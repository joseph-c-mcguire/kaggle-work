{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":3136,"databundleVersionId":26502,"sourceType":"competition"}],"dockerImageVersionId":30804,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-12-12T14:07:17.823798Z","iopub.execute_input":"2024-12-12T14:07:17.826545Z","iopub.status.idle":"2024-12-12T14:07:19.184338Z","shell.execute_reply.started":"2024-12-12T14:07:17.826434Z","shell.execute_reply":"2024-12-12T14:07:19.183056Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Baseline for Comparison \nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom tpot import TPOTClassifier\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/titanic/train.csv')\n\n# Feature Engineering: Create new features\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1\ndf['IsAlone'] = np.where(df['FamilySize'] == 1, 1, 0)\ndf['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf['Title'] = df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', \n                                    'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndf['Title'] = df['Title'].replace('Mlle', 'Miss')\ndf['Title'] = df['Title'].replace('Ms', 'Miss')\ndf['Title'] = df['Title'].replace('Mme', 'Mrs')\n\n# Define feature columns and target column\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'IsAlone', 'Title']\ntarget = 'Survived'\n\n# Split data into features (X) and target (y)\nX = df[features]\ny = df[target]\n\n# Preprocessing\n# Define numeric and categorical features\nnumeric_features = ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize']\ncategorical_features = ['Pclass', 'Sex', 'Embarked', 'IsAlone', 'Title']\n\n# Create preprocessing pipelines for both numeric and categorical data\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Preprocess the data\nX_preprocessed = preprocessor.fit_transform(X)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# Initialize TPOTClassifier\ntpot = TPOTClassifier(verbosity=2, generations=5, population_size=50, random_state=42, n_jobs=-1)\n\n# Train the TPOTClassifier\ntpot.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = tpot.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy}')\nprint('Classification Report:')\nprint(report)\n\n# Export the best model pipeline found by TPOT\ntpot.export('tpot_titanic_pipeline.py')","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T14:24:11.554878Z","iopub.execute_input":"2024-12-12T14:24:11.556343Z","iopub.status.idle":"2024-12-12T14:25:30.008621Z","shell.execute_reply.started":"2024-12-12T14:24:11.556280Z","shell.execute_reply":"2024-12-12T14:25:30.004244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score, classification_report\nfrom sklearn.ensemble import VotingClassifier\nfrom imblearn.over_sampling import SMOTE\n\n# Load the dataset\ndf = pd.read_csv('/kaggle/input/titanic/train.csv')\n\n# Feature Engineering: Create new features\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1\ndf['IsAlone'] = np.where(df['FamilySize'] == 1, 1, 0)\ndf['Title'] = df['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)\ndf['Title'] = df['Title'].replace(['Lady', 'Countess', 'Capt', 'Col', 'Don', 'Dr', \n                                    'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndf['Title'] = df['Title'].replace('Mlle', 'Miss')\ndf['Title'] = df['Title'].replace('Ms', 'Miss')\ndf['Title'] = df['Title'].replace('Mme', 'Mrs')\n\n# Define feature columns and target column\nfeatures = ['Pclass', 'Sex', 'Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'FamilySize', 'IsAlone', 'Title']\ntarget = 'Survived'\n\n# Split data into features (X) and target (y)\nX = df[features]\ny = df[target]\n\n# Preprocessing\n# Define numeric and categorical features\nnumeric_features = ['Age', 'SibSp', 'Parch', 'Fare', 'FamilySize']\ncategorical_features = ['Pclass', 'Sex', 'Embarked', 'IsAlone', 'Title']\n\n# Create preprocessing pipelines for both numeric and categorical data\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())\n])\n\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Combine preprocessing steps\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)\n    ])\n\n# Preprocess the data\nX_preprocessed = preprocessor.fit_transform(X)\n\n# Split data into training and testing sets\nX_train, X_test, y_train, y_test = train_test_split(X_preprocessed, y, test_size=0.2, random_state=42)\n\n# Handle class imbalance with SMOTE\nsmote = SMOTE(random_state=42)\nX_train_smote, y_train_smote = smote.fit_resample(X_train, y_train)\n\n# Define the model pipelines\nrf_model = Pipeline(steps=[\n    ('classifier', RandomForestClassifier(random_state=42))\n])\n\ngb_model = Pipeline(steps=[\n    ('classifier', GradientBoostingClassifier(random_state=42))\n])\n\n# Define the parameter grid for hyperparameter tuning\nparam_grid_rf = {\n    'classifier__n_estimators': [100, 200],\n    'classifier__max_depth': [10, 20, None],\n    'classifier__min_samples_split': [2, 5],\n    'classifier__min_samples_leaf': [1, 2, 4]\n}\n\nparam_grid_gb = {\n    'classifier__n_estimators': [100, 200],\n    'classifier__max_depth': [3, 4, 5],\n    'classifier__learning_rate': [0.01, 0.05, 0.1]\n}\n\n# Set up Grid Search\ngrid_search_rf = GridSearchCV(rf_model, param_grid_rf, cv=5, scoring='accuracy', n_jobs=-1)\ngrid_search_gb = GridSearchCV(gb_model, param_grid_gb, cv=5, scoring='accuracy', n_jobs=-1)\n\n# Train the models with Grid Search\ngrid_search_rf.fit(X_train_smote, y_train_smote)\ngrid_search_gb.fit(X_train_smote, y_train_smote)\n\n# Get the best models\nbest_rf_model = grid_search_rf.best_estimator_\nbest_gb_model = grid_search_gb.best_estimator_\n\n# Combine the models into a voting classifier\nvoting_clf = VotingClassifier(estimators=[\n    ('rf', best_rf_model),\n    ('gb', best_gb_model)\n], voting='soft')\n\n# Train the voting classifier\nvoting_clf.fit(X_train_smote, y_train_smote)\n\n# Predict on the test set\ny_pred = voting_clf.predict(X_test)\n\n# Evaluate the model\naccuracy = accuracy_score(y_test, y_pred)\nreport = classification_report(y_test, y_pred)\n\nprint(f'Accuracy: {accuracy}')\nprint('Classification Report:')\nprint(report)\n\n# Print best parameters\nprint('Best parameters found by grid search for RandomForest:', grid_search_rf.best_params_)\nprint('Best parameters found by grid search for GradientBoosting:', grid_search_gb.best_params_)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-12-12T14:21:18.387777Z","iopub.execute_input":"2024-12-12T14:21:18.388665Z","iopub.status.idle":"2024-12-12T14:21:56.534015Z","shell.execute_reply.started":"2024-12-12T14:21:18.388621Z","shell.execute_reply":"2024-12-12T14:21:56.532516Z"}},"outputs":[],"execution_count":null}]}